# 웹 크롤링

## 웹스크래핑
## 웹크롤링
: 두개는 같은 기술이다. 
스크래핑이 한페이지라면, 크롤링은 여러 페이지를 자동화.
구분을 잘 안하고, 같이 혼용한다. 크롤링이라고 범용적으로 말하자. 

: 여러 웹페이지를 돌아다니며 원하는 정보를 모으는 기술
: 원하는 정보를 추출하는 스크래핑과 여러 웹 페이지를 자동으로 탐색하는 크롤링의 개념을 합쳐 웹 크롤링이라고 부름
: 즉, 웹 사이트들을 돌아다니며 필요한 데이터를 추출할 수 있도록 자동화 된 프로세스


- 웹 크롤링 프로세스 
  - 웹페이지 다운로드
    : 해당 웹페이지의 HTML, CSS, JavaScript(역할: 페이지 자체를 바꿔주는 역할, 즉 동적인 부분을 자바스크립트가 담당) 등의 코드를 가져오는 단계
  - 페이지 파싱(// 문자열 파싱: 문자열에서 우리가 원하는 부분만 가져오는 것.)
    : 다운로드 받은 코드를 분석하고 필요한 데이터를 추출하는 단계
  - 링크 추출 및 다른 페이지 탐색
    : 다른 링크를 추출하고, 다음 단계로 이동하여 원하는 데이터를 추출하는 단계
    (ex. 페이징, 페이지를 나누어놨다. 페이지네이션)
  - 데이터 추출 및 저장-> (여기서 pandas와 web crawing을 이용할 수 있다.)
    : 분석 및 시각화에 사용하기 위해 데이터를 처리하고 저장하는 단계

- 웹 크롤링 실습
  [[1. 준비 단계]]
    : requests: HTTP 요청을 보내고 응답을 받을 수 있는 모듈
    request.get(URL)

    ->> GET method
    -> 조회, 즉 GET 요청을 보내는 거다.

    : BeautifulSoup : HTML문서에서 원하는 데이터를 추출하는데 사용되는 파이썬 라이브러리

    ->> 페이지 하나가 그냥 request로 가져오면, 복잡한 문자열로 가져왔었다.

    ->> 그래서 좀 더 파싱이 잘된 형태로 가져오기 위해서 이걸 쓰는 것.

    : Selenium : 웹 애플리케이션을 테스트하고, 자동화하기 위한 파이썬 라이브러리.

        : 웹페이지의 동적인 컨텐츠를 가져오기 위해 사용함(검색 결과 등)

    ```
    pip install requests beautifulsoup4 selenium
    ```


  [[2. 기본 예제 실습]]
     example.py
```
def crawling_basic():
    url = 'http://quotes.toscrape.com/tag/love/' 
        1. url을 사랑관련 문구가 있는 링크로 지정.
 
    response = requests.get(url)    
    2. requests.get(url): 해당 url로 요청을 보내겠다. 

    print('response = ' , response)

    html_text = response.text
    3. response.text안에 html문서가 들어있다. 

    # print(type(html_text)) 
    4. type을 찍어보면, string인것을 알 수 있다.--> 문자열은 구조도 없고,,, 문장안에서 잘라오기가 쉽지않음.
    -> 문자열은 활용이나 파싱이 쉽지않다 왜냐면, 구조화가 안되어 있어서. 
    따라서, BeautifulSoup(html_text, 'html.parser') 아래처럼 해준다.

    soup = BeautifulSoup(html_text, 'html.parser')
    # print(type(soup))   
    5. bs4.BeautifulSoup - BeautifulSoup가 제공하는 객체로 변환된것을 볼수 잇음

    # print(soup.prettify())
    6. 위의 내용을 들여쓰기가 들어간, 좀더 구조적으로 보기좋은 형태로 출력해줌.

    
    # 7- 1. 태그를 이용하여 하나 검색: find

    main = soup.find('a')
    print(f'제목 : {main}')-->>a태그 전체가 나옴,
    print(f'제목 : {main.text}')-->> a태그 안의 text 부분만 나옴.

    
    # 7- 2. 해당 태그인 모든 요소 검색: 모든 a태그들을 가져와서 리스트 형태로 반환함.
    
    a_tags = soup.find_all('a')
    # print(f'a 태그 : {a_tags}')
    # 리스트 형태로 반환하기 때문에, 반복문으로 출력할 수 있다!!!!!\

    # for tag in a_tags:
    #     print(f'태그: {tag.text}')


    # 위에보면 태그들은 반복된게 많아서, 하나만 택해서 가져오고 싶다.

->  # 7- 3. CSS 선택자로 하나 검색->> 예전에 했었다. css선택자.
    # 총 3가지방법이 있엇다. 태그:p, 클래스:. . id: #으로 선택을 했었다. 그리고 클래스는 단일로 선택을 권장함.
    -> 위에 나오는 find보다는, select나 select_one 이 두개를 더 많이 쓰게 된다. 
      왜냐면, 태그는 중복이 더 많으니까.
    
    # 선택자가 일치하는 첫 번째글
    
    # word = soup.select_one('.text')
    -> 택스트라는 클래스(.)가 붙어있는 첫번째 글

    # print(f'첫 번째 글 = {word.text}')

    # 7- 4. CSS 선택자로 여러 개 검색하기@@@가장 많이 쓰인다.
    words = soup.select('.text')
    #meme 밑에 라라라는 내가 만든 부분.
    # print(f'라라라{words}')
    
    마찬가지로 리스트 형태라서, 반복문을 돌릴 수 있다. 
    for w in words:
        print(f'글 : {w.text}')

#     # 예쁘게 출력하기
#     # print(soup.prettify())

```

  [[3. 구글 기본 예제]]
    google__crawling.py
```
def crawling_basic():


# 1. url에 검색결과가 해당되는 url을 바로가져온다. 

url = 'https://www.google.com/search?q=%ED%83%95%EC%88%98%EC%9C%A1&oq=%ED%83%95%EC%88%98%EC%9C%A1&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDMyMzlqMGoxqAIAsAIB&sourceid=chrome&ie=UTF-8'

# 2. requests의 get함수를 이용해 해당 url로 부터 html이 담긴 자료를 받아옴
response = requests.get(url)    
# print('response = ' , response)

# 우리가 얻고자 하는 html 문서가 여기에 담기게 됨
html_text = response.text

print(html_text)

# 3. with안에 들어가 있는 코드가 다 끝난다면, 그 이후에 메모리를 지워버려라. 효율적으로 관리할 수 있는 문법이다.
with open('soup.txt', 'w', encoding='utf-8') as file:
    file.write(html_text)

->> 자바 스크립트가 없어서, 우리가 원하는 형태로 출력되고 있지는 않음!!

  - request모듈은 정적인 부분, 즉 서버가 이미 가지고 있는 데이터만 다운로드 가능.
    => 동적인 컨텐츠를 다운로드 받을 수 없다! (탕수육이라는 결과를 통해서 변경되는 부분이 동적인 부분이다.)

    => 그럼 동적인 컨텐츠를 받을 수 있는 방법은? selenium
    selenium: 개발자들이 동적 웹 테스트를 위해서 많이 사용-> 동적으로 테스트를 하려면, 자동으로 페이지를 열어야하고, 잘열었는지 확인해야하는 이런게 있는데, 우리가 크롤링에서 활용. 


crawling_basic()

```

  
example1.py
```
실행하면, 우리가 아는 것과 비슷한 것이 출력됨.
즉, 동적인 부분들이 해결되고 난뒤를 우리가 볼 수 있게 됨.

즉, request는 브라우저를 열지 않으면 알수 없는 정적인 데이터만 
받아올 수 있다면, 

selenium은 실제로 열어버린다. 
    driver = webdriver.Chrome()
    driver.get(url)
열어서,
크롬페이지 긁어와서 파싱을 하는 거다.

```
example2.py
```

    result_stats = soup.select_one("div#result-stats")
내가 검색 결과와 관련된 것들을 보고 싶다고 했을때, div#result-stats를 사용하자. 그럼 검색결과를 꺼내올 수 있다.

```


example3.py
```
제목들마다 가져올래!
        title = g.select_one(".LC20lb.MBeuO.DKV0Md")


```




[참고]BeautifulSoup4 요소 선택 메서드 종류
  - find()
  : 태그를 사용하여 요소를 검색, 첫 번째로 일치하는 요소를 반환
  
  - find_all()
  : 태그를 사용하여 요소를 검색, 모든 일치하는 요소를 리스트로 반환
  
  - select()
  : CSS선택자를 사용하여 요소를 검색, 모든 일치하는 요소를 리스트로 반환
  
  - select_one()
  : CSS선택자를 사용하여 요소를 검색, 첫 번째로 일치하는 요소를 반환
  
  - find_parent() /find_next_sibling() / find_previous_silbling()
  : 태그를 사용하여 요소를 검색, 각각 일치하는 요소의 부모/다음 형제 요소/이전형제 요소를 반환
   
   + 공식 문서:
   https://beautiful-soup-4.readthedocs.io/en/latest/

## Django에서 활용하기


```
실습

    #1. 크롤링ㅣ:: 검색을 할 수 잇도록 http://127.0.0.1:8000/crawlings/화면을 구성
    #2. DB에 저장-> 중복 제거
    #3. 제목출력

```

1. models.py
```
class Article(models.Model):
    query = models.TextField()
    title = models.TextField()
```
+ 추가 설명 : 테이블 쪼개기
```

# +
class Article(models.Model):
    title = models.TextField()

# 하나의 쿼리로 여러개의 게시글을 검색 할 수 잇음
class Article(models.Model):
    name = models.TextField()
```


```
rm db.sqlite3

python manage.py makemigrations

```

crawling> views.py에서 
사용자가 검색을 하면 크롤링을 진행하는 부분을 만들어 주기.
```
1. 검색하는 입력을 받는 부분을 
index.html에서 만들어 주기


<form action="{% url "crawlings:index" %}" method = "POST">
  {% csrf_token %}
  <label for="query">검색어</label>
  <input type="text" id="query" name="query" >
  <button>검색하기</button>
  {% comment %} <input type="submit" value = "검색하기"> {% endcomment %}
  {% comment %} -> 버튼을 써도, input을 써도 상관없는데,
  button이 간단하고, form태그안에 잇으면 자동으로  submit을 해준다. {% endcomment %}
</form>

```

```
2. views.py에서 크롤링하기
def index(request):
    # 사용자가 검색을 하면, 크롤링을 진행
    if request.method == 'POST':
        results = [] #검색 결과를 모으기
        # 아까 index.html에서 query라는 이름으로 인풋을 담아줬으니
        query = request.POST.get('query')
        # 크롤링 로직을 쿠리를  통해 하는거를 , example3번에 이미 만들어 놓은것을 가져오자. 
        #(함수만 다 들고 오자)
        titles = 


- 가져온 example3.py 약간 수정하기
titles = []
    # 해당 요소를 반복하며
    for g in g_list:
        # 요소 안에 LC20lb MBeuO DKV0Md 클래스를 가진 특정 요소 선택
        title = g.select_one(".LC20lb.MBeuO.DKV0Md")
        # 요소가 존재 한다면
        if title is not None:
            title_text = title.text
            # print('제목 = ', title_text)
            titles.append(title_text)
    return titles


```

```
def index(request):
    # 사용자가 검색을 하면, 크롤링을 진행
    if request.method == 'POST':
        results = [] #검색 결과를 모으기
        # 아까 index.html에서 query라는 이름으로 인풋을 담아줬으니
        query = request.POST.get('query')
        # 크롤링 로직을 쿠리를  통해 하는거를 , example3번에 이미 만들어 놓은것을 가져오자. 
        #(함수만 다 들고 오자)
        titles = get_google_data(query)
        # print(titles)

        for title in titles:
            # #Article을 저장하는 것(단, 중복을 제거)
            # Article.objects.create(query= query, title = title)

            # # (실행할 때 마다 같은 데이터 를 저장할 수는 업으니까,)중복을 제거하자
            # if not Article.objects.filter(query=query, title= title).exists():
            #     Article.objects.create(query= query, title = title)

            # 있으면 가져오고, 없으면 저장해라-> 이걸 하는 메서드는?
            article, created_article = Article.objects.get_or_create(query=query, title= title)


    # 위에꺼를 그대로 출력하기 위해서, 밑에서 가져와서(~.all) 저장해라. 
    context = {
        'results': Article.objects.all()
    }
    #-> index.html가서 출력하기

    # 아니라면, 그냥 검색 페이지를 제공
    return render(request, 'crawlings/index.html')
```

 #-> index.html가서 출력하기
```

{% for result in results %}
  <p>{{forloop.counter }}{{result.title}}</p>
{% endfor %}
```

[한걸음더]

인증된 유저만 크롤링 가능
index.html에서 하자.

user개념은 항상 request객체에 다 들어 있다. 
request는 templates나 view에서 모두 활용할 수 있는 변수이다.
```
{% extends "base.html" %}

{% block content %}

<h1>메인 페이지</h1>

{% comment %} {{ request  }} {% endcomment %}

{% if request.user.is_authenticated %}
  <form action="{% url "crawlings:index" %}" method = "POST">
    {% csrf_token %}
    <label for="query">검색어</label>
    <input type="text" id="query" name="query" >
    <button>검색하기</button>
    {% comment %} <input type="submit" value = "검색하기"> {% endcomment %}
    {% comment %} -> 버튼을 써도, input을 써도 상관없는데,
    button이 간단하고, form태그안에 잇으면 자동으로  submit을 해준다. {% endcomment %}
  </form>

  {% for result in results %}
    <p>{{forloop.counter }}{{result.title}}</p>
  {% endfor %}
{% else %}  
  <p>먼저 로그인 해 주세요.</p>
{% endif %}

{% endblock content %}

```
is_authenticated는 보다싶히
index.html에서 작업도 가능하고(프론트),

views.py에서 작업도 가능하다(백엔드)

그렇다면, 어디에서 해줘야 할까?

-> 둘다 해줘야한다. 프론트 쪽에서는 사용자에게 안보이도록 만들어야하고
벡엔드에서는 사용자들이 직접요청을 해버리는 경우를 막기위해.

=> >> 프로젝트시에는 둘다 하도록.

-----------------------

구현할때에는 교재최대한 보지말고,

검색+ 공식문서를 통해서 만들어야 실력이 는다 !!!!


# 브랜치

## 목차
### 기초지식
### Branch


## 기초지식
```bash
# 이 폴더를  git오로 관리 하겠다.
$ git init
```

- 현재 작업중인 폴더 내의 .git을 제외한 폴더 위치를  'working directory'

- 'working directory' 에만 작성한 내용은 아직 git이 어떤 변동사항이 있는지 모른다.


```bash
# staging area에 해당하는 다일을 등록
$ git add { file_path }

```

- 'staging area' 란, 등록한 파일들의 변동사항만 기록한 txt 파일을 '.git'폴더 어딘가에 모아둔다,

```bash
# staging area에 등록해둔 변동사항을 하나의 버전으로 등록한다.
# 그후 staging area는 비운다.
$ git commit -m "commit message"
```


- 'commit' 즉, 버전이란 아까전 'staging area' 에 등록해준 변동사항만을 모아서 하나의 상태로 저장한다.,
- 그곳을 'repository'라고 부른다

--  여기까지 내 컴퓨터---

ex) 신발의 일부를 만들어 서 단상에 올리는게 add,
사진을 찍어올리고,,
레포짓에 버전1(밑창), 버전2(윗동), 버전3(신발끈)을 저장.
-> 버전 1, 2, 3 을 합치면 하나의 신발완성


repository = remote repository(원격레포짓)


git push origin master
: origin:저장소의 이름
: master: 이게 브런치 이름이다.!! 오늘은 여기 !!


------------------

## Branch

### git branch
: 나뭇가지 처럼 여러 갈래로 작업 공간을 나누어
독립적으로 작업할 수 있도록 도와주는 Git의 도구.

: 장점:
  1. 독립된 개발 환경을 형성하기 때문에 원본(master)에 대해 안전
  2. 하나의 작업은 하나의 브랜치로 나누어 진행되므로, 체계적으로 협업과 개발이 가능
  3. 손쉽게 브랜치를 생성하고 브랜치 사이를 이동할 수 있음.

### branch를 꼭 사용해야 할까?
  - 만약 상용중인 서비스에 발생한 에러를 해결하려면?
    1. 브런치를 통해 별도의 작업 공간을 만든다.
    2. 브랜치에서 에러가 발생한 버전을 이전 버전으로 되돌리거나 삭제한다.
    3. 브랜치는 완전하게 독립 되어있어서 작업 내용을 master브랜치에 아무런 영향을 끼치지 못한다.
    4. 이후 에러가 발생했다면?
    ...?


### branch를 만들어보자!

- checkout 말고..
branch 랑 switch를 이용해보자!

git branch : 브랜치 목록 확인

git branch -r : 원격 저장소의 브랜치 목록 확인

git branch <브랜치 이름> : 새로운 브랜치 생성

git branch -d <브랜치 이름> : 브랜치 삭제(병합된 프랜치만 삭제 가능)

git branch -D <브랜치 이름> : 브랜치 삭제(강제 삭제)

  -> 우린 D는 안쓴다. 없는 명령어로 치자.


----------------
필기 내용
# branch

git switch som
하고나서 
git log를 찍어보면,
HEAD-> 가 어디를가르키고 잇는지를 통해 알 수 있다.





[실습]


git init

touch README.md

git add .

git commit -m "first commit"

git log

git branch

git branch som

git branch

git switch som

git log

__ 여기서 README파일 수정하고~


git add README.md

git commit -m "som 2 작업함"

git log --oneline

git switch master



[2교시]
touch master.md
git status
git add master.md
git commit -m "master.md 추가"
git log
git log --oneline
git switch som
git log --oneline
git switch master
git merge som
git log --oneline
git branch -d som
git branch
git branch somsom
git log --oneline
touch test.md
git add test.md
git commit -m "새로만듬"
git log --oneline
git switch master
git merge somsom
git log --oneline
git log --oneline --graph

git branch -d somsom 
git branch
git branch min
git branch
git switch min 
git add .
git commit -m "min doing"
git switch master
git merge min
git add .
git commit -m "notice"
git merge min
git add .
git commit -m"conflict complete"
git log --oneline

원격을 추가하기 
git remote add origin 깃허브 복사한 주소
git push origin master
git branch

git branch front_login

git switch front_login
touch login.html
git add login.html
git commit -m "login completed"
git push origin front_login

git switch master
git switch front_login

git remote -v
git fetch origin master
git pull origin master

git log --oneline
git switch master
git pull origin master
git branch -d front_login





->>
 git add README.md
하기전에 README.md에서 변경사항을 만들자.


---
2교시

폴더가 working area이다.

merge
:q



```
[실습]= 전체 요약 정리
협업을 해보자!

협업
1. 두사람이 한팀이 되어, 한사람의 계정에서만 new 프로젝트를 생성한다
2. 각자의 컴퓨터에 폴더를 하나 만들고, "그 폴더 안에서 vscode"로 연다
2. 그 폴더 안에서 git bash를 열고, git clone "주소" 를 한다.
그리고 cd 05-pjt/ 해서 이동하여서 (master)가 보이도록 한다. 
3. 위의 깃배쉬 안에서 각자의 이름으로, 각자의 가지를 만든다
  1-1.
  (git branch one)
  git branch min

  1-2. 내브런치로 이동한다.
  git switch min

  1-3. 실제로 열린 "그 폴더 안에서 vscode"안에서 프로젝트를 만들고
  앱을 만들고, html을 만들고, 수정하는 작업들을 한다
  ((단, 영역을 나눠서, 두사람이 같은 파일을 수정하지 않게한다.
  두사람이 같은 파일을 수정하면, conflict가 나서 
  파일을 합치는 것이 힘들다))

  1-4. 
  깃배쉬에서 내브런치인 상태로,
  글작성을 다했으면,
  git add .
  git commit -m "first make"
  git push origin min

  1-5.
  원격(웹상의 팀 레포짓)에 가서 1-4를 한사람이
  새로 고침하면, create merge가 뜨는데 누른다
  그리고 merge를 또 누른다.

  그러고나면, 친구는 아무것도 하지 않고 있던 친구는

  1-6
  깃 배쉬에서 
  git switch master(마스터로가서)
  git pull origin master

  git switch min(본인 브랜치로 가서)
  git merge master



  ****--- 만약 둘다 동시에 수정을 시작했다면?
  친구1이 1-4, 1-5를하고, 1-6을 하면, 


  친구2가 친구1의 결과물을 받기 위해서는, 자신이 만든 것을 1-4, 1-5를 해야 1-6을 받을 수 있다.

### 고로 전체 순서는

친구 1의 1-4, 1-5,
친구 2의 1-4, 1-5, 1-6,
친구 1의 1-6을 하면

서로서로 변경된 모든것을 자신의 branch에 받아올 수 있게 된다.


삭제는? 안되나?



```